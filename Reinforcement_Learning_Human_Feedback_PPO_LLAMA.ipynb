{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SilvanaJ90/udemy_-machine_learning_ds/blob/main/Reinforcement_Learning_Human_Feedback_PPO_LLAMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cb7b12c"
      },
      "source": [
        "# Reinforcement Learning from Human Feedback con PPO sobre TinyLLAMA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42e9ea52"
      },
      "source": [
        "<div style=\"background-color:#D9EEFF;color:black;padding:2%;\">\n",
        "<h2>Enunciado del caso práctico</h2>\n",
        "\n",
        "En este caso práctico, se propone al alumno la realización de Reinforcement Learning from Human Feedback para evitar la generación de contenido tóxico sobre una versión reducida de LLAMA denominada [TinyLLAMA](https://huggingface.co/PY007/TinyLlama-1.1B-Chat-v0.3)\n",
        "\n",
        "Por oto lado, como algoritmo de recompensa (Reward model) se propone el uso de una versión de [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta) con fine-tuning para la detección de comportamiento tóxico/hate: https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "piqasq7m3dRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "831d29b1"
      },
      "source": [
        "# Resolución del caso práctico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V8dgd_BUeOK"
      },
      "source": [
        "## 0. Instalación de librerías externas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate peft bitsandbytes transformers trl xformers trl evaluate sentencepiece"
      ],
      "metadata": {
        "id": "vPIR50tPH6rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjNFCLMHWCXs"
      },
      "source": [
        "## 1. Lectura del modelo y del tokenizador"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Descarga del modelo y del tokenizador"
      ],
      "metadata": {
        "id": "KalsBY6woqmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para reducir el consumo de recursos copmutacionales, sobre todo memoria RAM, durante el proceso de re-entrenamiento y Reinforcement Learning vamos a aplir QLoRA sobre el modelo."
      ],
      "metadata": {
        "id": "SnFaFP9goGzb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdK3d284lxFw"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Definimos los paramétros para bitsandbytes\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNfU5_BKOTp1"
      },
      "outputs": [],
      "source": [
        "# Nombre del modelo\n",
        "model_name = \"PY007/TinyLlama-1.1B-Chat-v0.3\"\n",
        "\n",
        "# Leemos el modelo pre-entrenado el modelo LLAMA2-7b-chat\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0},\n",
        "    low_cpu_mem_usage=True # Reduccion del consumo de cpu y memoria al leer el modelo\n",
        ")\n",
        "\n",
        "CHAT_EOS_TOKEN_ID = 32002"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAl7HAdNPjDT"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Leemos el tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "652jxcqawjQk"
      },
      "source": [
        "### 1.2. Generación de texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ErX-b24n4Ll"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Creamos un pipeline para la tokenización y generación del texto\n",
        "tinyllama_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    num_return_sequences=1,\n",
        "    repetition_penalty=1.1,\n",
        "    max_new_tokens=200,\n",
        "    eos_token_id=CHAT_EOS_TOKEN_ID,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DCvtETXWJJT"
      },
      "outputs": [],
      "source": [
        "prompt = \"Actúa como si fueses el mayor experto en historia del mundo. Describe \\\n",
        "en pocas palabras lo que ocurrió en la segunda guerra mundial.\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "\n",
        "print(prompt_template)"
      ],
      "metadata": {
        "id": "U0gJLc0DJo79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pelMVjGATwUO"
      },
      "outputs": [],
      "source": [
        "# Invocamos el pipeline para realizar generación de texto\n",
        "output = tinyllama_pipe(prompt_template)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01t6CnUB_Lns"
      },
      "source": [
        "## 2. Selección y preparación del conjunto de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para este caso práctico vamos a utilizar un conjunto de datos denominado [Dialogsum](https://huggingface.co/datasets/knkarthick/dialogsum):\n",
        "\n",
        "DialogSum es un conjunto de datos de resumen de diálogos a gran escala, compuesto por 13.460 diálogos divididos en entrenamiento, prueba y validación.\n",
        "\n",
        "Ejemplo del conjunto de datos:\n",
        "\n",
        "```\n",
        "{'id': 'train_0', 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\", 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\", 'topic': \"get a check-up}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "fJqqeQ3gv-aa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBM_wgklBoJE"
      },
      "source": [
        "### 2.1. Lectura del conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"knkarthick/dialogsum\")"
      ],
      "metadata": {
        "id": "Y44NC6D8sz2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "id": "TWAT_fRtxUcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reducimos el conjunto de datos\n",
        "NUM_EJ_TRAIN = 1000\n",
        "NUM_EJ_VAL = 100\n",
        "NUM_EJ_TEST = 100\n",
        "\n",
        "# Subconjunto de entrenamiento\n",
        "ds['train'] = ds['train'].select(range(NUM_EJ_TRAIN))\n",
        "\n",
        "# Subconjunto de validación\n",
        "ds['validation'] = ds['validation'].select(range(NUM_EJ_VAL))\n",
        "\n",
        "# Subconjunto de pruebas\n",
        "ds['test'] = ds['test'].select(range(NUM_EJ_TEST))"
      ],
      "metadata": {
        "id": "2uKsPh3o8zJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds['train']['dialogue'][2])"
      ],
      "metadata": {
        "id": "9Afvjeve9A8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Preparación del conjunto de datos para proporcionarlo al algoritmo"
      ],
      "metadata": {
        "id": "I_XW2E-KCp87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_dataset(dataset, tokenizer, input_min_text_length, input_max_text_length):\n",
        "\n",
        "    # Filtramos los dialogos que se encuentran entre el tamaño minimo y maximo\n",
        "    dataset[\"train\"] = dataset[\"train\"].filter(lambda x: len(x[\"dialogue\"]) > input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length, batched=False)\n",
        "    dataset[\"validation\"] = dataset[\"validation\"].filter(lambda x: len(x[\"dialogue\"]) > input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length, batched=False)\n",
        "    dataset[\"test\"] = dataset[\"test\"].filter(lambda x: len(x[\"dialogue\"]) > input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length, batched=False)\n",
        "\n",
        "    def tokenize(sample):\n",
        "        # Plantilla de entrenamiento para cada ejemplo\n",
        "        prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{sample[\"dialogue\"]}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "        sample[\"input_ids\"] = tokenizer.encode(prompt)\n",
        "        # Esto debe llamarse \"query\", es un requisito de la biblioteca PPO\n",
        "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
        "        return sample\n",
        "\n",
        "    # Tokenizamos cada dialogo\n",
        "    dataset = dataset.map(tokenize, batched=False)\n",
        "\n",
        "    # Convertimos el conjunto de datos a un formato adecuado\n",
        "    dataset.set_format(type=\"torch\")\n",
        "\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "KYGeGTaCtCbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = prep_dataset(ds, tokenizer, input_min_text_length=200, input_max_text_length=1024)"
      ],
      "metadata": {
        "id": "k-Sg2zOgy95t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds[\"train\"][\"query\"][0])"
      ],
      "metadata": {
        "id": "uiEhhyf-tO-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFVyogGn0ruV"
      },
      "source": [
        "## 3. Configuración Reinforcement Learning from Human Feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKOiXMJBMBPD"
      },
      "source": [
        "### 3.1. Configuración de LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnDnMYV-HYtW"
      },
      "source": [
        "La siguiente función es interesante para comparar el número de parámetros entrenables que tiene el modelo antes y después de apalicar LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWyRL5IhUpu3"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"\\ntrainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZ7xJDTFUqtF"
      },
      "outputs": [],
      "source": [
        "print(print_trainable_parameters(model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxiTkz4zy8aK"
      },
      "source": [
        "Configuramos LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-il4eI-L4yg"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training\n",
        "\n",
        "# Definición de la configuración de LoRA\n",
        "lora_config = LoraConfig(\n",
        "                 r = 16, # Dimensión de las matrices\n",
        "                 lora_alpha = 16, # LoRA scaling factor\n",
        "                 lora_dropout = 0.05, # Regularización\n",
        "                 bias=\"none\",\n",
        "                 task_type=\"CAUSAL_LM\" # Tipo de tarea/modelo al que aplicarlo\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtqgSmk7zB5U"
      },
      "outputs": [],
      "source": [
        "# Aplicamos la configuración al modelo\n",
        "model_peft = get_peft_model(model, lora_config)\n",
        "\n",
        "# Mostramos el número de parámetros que se van a entrenar\n",
        "model_peft.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXTs5vb_RTQp"
      },
      "source": [
        "### 3.2. Configuración (Proximal Policy Optimization)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Durante el proceso de PPO, sólo se actualizarán algunos parámetros. En concreto, los parámetros entrenables con LoRA junto con algunos parámetros adicionales. Puedes encontrar más información sobre esta clase de modelos en [su documentación](https://huggingface.co/docs/trl/main/en/models#trl.create_reference_model).\n",
        "\n",
        "El número de parámetros entrenables puede calcularse como `(𝑛+1)∗𝑚`\n",
        " donde `𝑛` es el número de unidades de entrada (aquí `𝑛=2048`) y `𝑚` es el número de unidades de salida (aquí `𝑚=1`). El término `+1` en la ecuación tiene en cuenta el término bias.\n",
        "\n",
        "E nuestro caso, el número de parámetros entrenables debe ser: `2,252,800 + 2.049 = 2.254.849 parámetros`"
      ],
      "metadata": {
        "id": "05LCMPE9NAvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import AutoModelForCausalLMWithValueHead\n",
        "\n",
        "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(model_peft,\n",
        "                                                              torch_dtype=torch.bfloat16,\n",
        "                                                              is_trainable=True,\n",
        "                                                              device_map={\"\": 0},\n",
        ")\n",
        "\n",
        "print(f'Parametros entrenables PPO Model:\\n{print_trainable_parameters(ppo_model)}\\n')\n",
        "print(ppo_model.v_head)"
      ],
      "metadata": {
        "id": "tHGIu3lpMkYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tal y como hemos comentado en secciones anteriores, además del modelo que vamos a ir ajustando en el proceso de Reinforcement Learning, se requiere una instancia del mismo modelo con los parámetros congelados para que sirva de referencia y calcular las probabilidades relativas de los tokens generados.\n",
        "\n",
        "El modelo de referencia representará el LLM antes de la \"desintoxicación\". Ninguno de los parámetros del modelo de referencia se actualizará durante el entrenamiento utilizando PPO."
      ],
      "metadata": {
        "id": "R4UYubHiOHrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import create_reference_model\n",
        "\n",
        "ref_model = create_reference_model(ppo_model)\n",
        "\n",
        "print(f'Parámetros entrenables modelo de referencia:\\n{print_trainable_parameters(ref_model)}\\n')"
      ],
      "metadata": {
        "id": "ecK3Db5JOISo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Creación del Reward Model"
      ],
      "metadata": {
        "id": "BTKiMGenO5rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo siguiente que debemos hacer es selccionar el modelo de reocmpensas (Reward model).\n",
        "\n",
        "Para este caso práctico vamos a hacer uso de una versión de [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta) con fine-tuning que ha creado Meta (Facebook) para la detección de comportamiento tóxico/hate: https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target\n",
        "\n",
        "El modelo predecirá las probabilidades de que un texto pertenezca a una de las dos clases: `(no_hate, hate)`"
      ],
      "metadata": {
        "id": "V6EUWaUwO-ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "reward_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
        "\n",
        "# Cargamos el modelo\n",
        "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    reward_model_name, device_map=\"auto\")\n",
        "\n",
        "# Cargamos el tokenizador\n",
        "reward_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    reward_model_name, device_map=\"auto\")\n",
        "\n",
        "# Etiquetas del modelo\n",
        "print(f\"\\nEtiquetas del modelo: {reward_model.config.id2label}\")"
      ],
      "metadata": {
        "id": "xOV5z98sPAvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación se muestra como funcionaría el proceso de generación de la recompensa."
      ],
      "metadata": {
        "id": "KMmyhGgJ3O1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reward_evaluation(text):\n",
        "\n",
        "  toxicity_input_ids = reward_tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "  logits = reward_model(input_ids=toxicity_input_ids.to('cuda')).logits\n",
        "  print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
        "\n",
        "  # Mostramos las probabilidades para cada categoria: [not hate, hate]\n",
        "  probabilities = logits.softmax(dim=-1).tolist()[0]\n",
        "  print(f'probabilities [not hate, hate]: {probabilities}')\n",
        "\n",
        "  # Mostramos la recompensa\n",
        "  not_hate_index = 0\n",
        "  nothate_reward = (logits[:, not_hate_index]).tolist()\n",
        "  print(f'reward (high): {nothate_reward}')"
      ],
      "metadata": {
        "id": "SNwT1w5fPloG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Persona 1# le dice a Juan que no ha visto la pelicula.\n",
        "non_toxic_text = \"#Person 1# tells Tommy that he didn't like the movie.\"\n",
        "\n",
        "reward_evaluation(non_toxic_text)"
      ],
      "metadata": {
        "id": "T8BuZzBARbnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Persona 1# le dice a Tommy que la película era terrible, tonta y estúpida.\n",
        "toxic_text = \"#Person 1# tells Tommy that the movie was terrible, dumb and stupid.\"\n",
        "\n",
        "reward_evaluation(toxic_text)"
      ],
      "metadata": {
        "id": "7gNNtMFvRlbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVY45QhS8G4W"
      },
      "source": [
        "## 4. Aplicación del Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Lectura del conjunto de datos"
      ],
      "metadata": {
        "id": "V8p9KrQ84pn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para la lectura de los datos por parte del POO, necesitamos definir un data collator que transforme el formato original en un formato específico"
      ],
      "metadata": {
        "id": "Gn8umtN-4x-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collator(data):\n",
        "    return dict((key, [d[key] for d in data]) for key in data[0])"
      ],
      "metadata": {
        "id": "9WReXUQpUxsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = [{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}]\n",
        "\n",
        "print(f'Collator input: {test_data}')\n",
        "print(f'Collator output: {collator(test_data)}')"
      ],
      "metadata": {
        "id": "hclZuUFQ48S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. Configuración de los parámetros para el entrenamiento"
      ],
      "metadata": {
        "id": "I6MIrpqMU-dU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import PPOConfig, PPOTrainer\n",
        "\n",
        "learning_rate=1.41e-5\n",
        "max_ppo_epochs=1\n",
        "mini_batch_size=2\n",
        "batch_size=2\n",
        "\n",
        "config = PPOConfig(\n",
        "    # model_name=model_peft,\n",
        "    learning_rate=learning_rate,\n",
        "    ppo_epochs=max_ppo_epochs,\n",
        "    mini_batch_size=mini_batch_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "ppo_trainer = PPOTrainer(config=config,\n",
        "                         model=ppo_model,\n",
        "                         ref_model=ref_model,\n",
        "                         tokenizer=tokenizer,\n",
        "                         dataset=ds[\"train\"],\n",
        "                         data_collator=collator)"
      ],
      "metadata": {
        "id": "GEaNGN32U5VU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3. Reinforcement Learning (Fine-tuning)"
      ],
      "metadata": {
        "id": "AnrvVzHdV32-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este punto vamos a entrar en un bucle en el que se irán actualizando los valores de los parámetros del modelo utilizando PPO.\n",
        "\n",
        "El bucle consiste en los siguientes pasos principales:\n",
        "\n",
        "1.   Obtener los completions de LLM que se esta ajustando (modelo PEFT).\n",
        "2.   Obtener los sentimientos para las respuestas del modelo utilizando RoBERTa\n",
        "3.   Optimizar el valor de los parámetros del LLM con PPO utilizando el trío (consulta, respuesta, recompensa).\n",
        "\n",
        "La operación se está ejecutando correctamente si ves aparecer las siguientes métricas:\n",
        "\n",
        "* `objective/kl`: Este valor se refiere a la divergencia de Kullback-Leibler (KL) entre las distribuciones de probabilidad del modelo re-entrenado y el modelo de referencia. Una divergencia KL baja sugiere que las actualizaciones de los parámetros no están cambiando drásticamente la política, lo cual es generalmente bueno para la estabilidad del entrenamiento.\n",
        "* `ppo/returns/mean`: Este valor representa la recompensa promedio que el agente está obteniendo. En el aprendizaje por refuerzo, el objetivo es generalmente maximizar la recompensa total, por lo que queremos ver este número aumentar a lo largo del tiempo.\n",
        "* `ppo/policy/advantages_mean`: Este valor se refiere a la función de ventaja, que mide cuánto mejor (o peor) es tomar una acción específica en un estado específico, en comparación con la acción promedio en ese estado. Un valor de ventaja positivo sugiere que la acción es mejor que el promedio, y un valor negativo sugiere que es peor. Al maximizar la función de ventaja promedio, el algoritmo busca mejorar la política para obtener mejores recompensas."
      ],
      "metadata": {
        "id": "IvmmhG7QV6uK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_pipe = pipeline(\"sentiment-analysis\",\n",
        "                          tokenizer=reward_tokenizer,\n",
        "                          model=reward_model_name,\n",
        "                          device=0) # GPU\n",
        "\n",
        "# Argumentos proporcionados para la produción de la recompensa\n",
        "reward_kwargs = {\n",
        "    \"top_k\": None, # Return all scores.\n",
        "    \"function_to_apply\": \"none\", # Set to \"none\" to retrieve raw logits.\n",
        "    \"batch_size\": 2,\n",
        "    \"padding\":'max_length',\n",
        "    \"truncation\": True,\n",
        "}"
      ],
      "metadata": {
        "id": "vX5U_hEN9Dq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentiment_pipe(non_toxic_text, **reward_kwargs))"
      ],
      "metadata": {
        "id": "eeEffEeM9EXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl.core import LengthSampler\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "output_min_length = 100\n",
        "output_max_length = 300\n",
        "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
        "\n",
        "# Argumentos proporcionados para la generación\n",
        "generation_kwargs = {\n",
        "    \"min_length\": 5,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "}\n",
        "\n",
        "# Número de iteraciones durante el prceso de RL\n",
        "max_ppo_steps = 15\n",
        "\n",
        "for step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
        "    # Terminamos el bucle cuando alcanzamos el máximo de iteraciones\n",
        "    if step >= max_ppo_steps:\n",
        "        break\n",
        "\n",
        "    print(f\"\\nIteración {step} del proceso de Reinforcement Learning...\")\n",
        "    # Leemos los prompts de entrada para realizar la generación\n",
        "    prompt_tensors = batch[\"input_ids\"]\n",
        "\n",
        "    # Generamos las completions del LLM (TinyLLAMA)\n",
        "    summary_tensors = []\n",
        "    for prompt_tensor in prompt_tensors:\n",
        "        print(\"Procesando prompt...\")\n",
        "        max_new_tokens = output_length_sampler()\n",
        "        generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n",
        "        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n",
        "        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n",
        "\n",
        "    # Destokenizamos los completions. Este campo debe llamarse \"response\"\n",
        "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in summary_tensors]\n",
        "\n",
        "    # Mostramos por pantalla las completions\n",
        "    print(f\"Completions: {batch['response']}\\n\")\n",
        "\n",
        "    # Calculamos la recompensa para los completions generados\n",
        "    query_response_pairs = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
        "    rewards = sentiment_pipe(query_response_pairs, **reward_kwargs)\n",
        "\n",
        "    # Calculamos la recompensa a partir del valor \"not_hate\"\n",
        "    not_hate_index = 0\n",
        "    reward_tensors = [torch.tensor(reward[not_hate_index][\"score\"]) for reward in rewards]\n",
        "\n",
        "    # Ejecutamos un paso de optimización de los parámetros de TinyLLAMA con PPO\n",
        "    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n",
        "    ppo_trainer.log_stats(stats, batch, reward_tensors)\n",
        "\n",
        "    print(f'\\nobjective/kl: {stats[\"objective/kl\"]}')\n",
        "    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n",
        "    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')\n",
        "    print('-'.join('' for x in range(100)))"
      ],
      "metadata": {
        "id": "xgPkknJsVm5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Guardamos el modelo en disco"
      ],
      "metadata": {
        "id": "l9n_oOmkICuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardamos el modelo en disco\n",
        "ppo_model.save_pretrained(\"/content/drive/MyDrive/TinyLLAMA-ppo\")"
      ],
      "metadata": {
        "id": "NyNJ3F38HAWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYmLeJXAH6k_"
      },
      "source": [
        "## 5. Generación de texto con TinyLLAMA con RLHF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo del conjunto de pruebas\n",
        "print(ds[\"test\"][\"dialogue\"][10])"
      ],
      "metadata": {
        "id": "xw6eSFChD_Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nos aseguramos de que el modelo esta en la GPU\n",
        "ppo_model = ppo_model.to('cuda')\n",
        "\n",
        "# Nos aseguramos de que el tensor de entrada esta en el formato correcto\n",
        "input_ids = torch.as_tensor(ds['test']['input_ids'][10], dtype=torch.long).unsqueeze(dim=0).to('cuda')\n",
        "\n",
        "# Argumentos proporcionados para la generación\n",
        "generation_kwargs = {\n",
        "    \"min_length\": 5,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "    \"max_new_tokens\": 150,\n",
        "    \"input_ids\": input_ids\n",
        "}\n",
        "\n",
        "# Generamos la predicción\n",
        "summary = ppo_model.generate(**generation_kwargs)\n",
        "\n",
        "# Decodificamos la predicción\n",
        "print(tokenizer.decode(summary.squeeze()))"
      ],
      "metadata": {
        "id": "vXCeuDFzjOrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Leemos el modelo de disco"
      ],
      "metadata": {
        "id": "iFS2KgifG610"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBGVT81_07RG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model_name = \"PY007/TinyLlama-1.1B-Chat-v0.3\"\n",
        "adapters_name = \"/content/drive/MyDrive/TinyLLAMA-ppo\"\n",
        "\n",
        "print(f\"Cargando el modelo: '{model_name}' en memoria...\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    #load_in_4bit=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(model, adapters_name)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "print(f\"El modelo: '{model_name}' ha sido cargado correctamente\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Leemos el tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "fkugO4H8HlzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "CHAT_EOS_TOKEN_ID = 32002\n",
        "\n",
        "# Creamos un pipeline para la tokenización y generación del texto\n",
        "tinyllama_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    num_return_sequences=1,\n",
        "    repetition_penalty=1.1,\n",
        "    max_new_tokens=200,\n",
        "    eos_token_id=CHAT_EOS_TOKEN_ID,\n",
        ")"
      ],
      "metadata": {
        "id": "me-YzlbLIev3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"#Person1#: What's wrong with you? Why are you scratching so much?\n",
        "#Person2#: I feel itchy! I can't stand it anymore! I think I may be coming down with something. I feel lightheaded and weak.\n",
        "#Person1#: Let me have a look. Whoa! Get away from me!\n",
        "#Person2#: What's wrong?\n",
        "#Person1#: I think you have chicken pox! You are contagious! Get away! Don't breathe on me!\n",
        "#Person2#: Maybe it's just a rash or an allergy! We can't be sure until I see a doctor.\n",
        "#Person1#: Well in the meantime you are a biohazard! I didn't get it when I was a kid and I've heard that you can even die if you get it as an adult!\n",
        "#Person2#: Are you serious? You always blow things out of proportion. In any case, I think I'll go take an oatmeal bath.\"\"\""
      ],
      "metadata": {
        "id": "Surp4mr4NIqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{prompt}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "print(prompt_template)"
      ],
      "metadata": {
        "id": "4lt-Z00kM3xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invocamos el pipeline para realizar generación de texto\n",
        "output = tinyllama_pipe(prompt_template)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "6Bm6cMn4IpFY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}